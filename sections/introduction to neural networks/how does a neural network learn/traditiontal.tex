The traditional approach of optimizing the connection weigths to improve the network's accuracy is named backpropagation.

\begin{quote}
\emph{"The Backpropagation algorithm is a supervised learning method for multilayer feed-forward networks from the field of Artificial Neural Networks. }

\emph{Feed-forward neural networks are inspired by the information processing of one or more neural cells, called a neuron. A neuron accepts input signals via its dendrites, which pass the electrical signal down to the cell body. The axon carries the signal out to synapses, which are the connections of a cell’s axon to other cell’s dendrites."} \cite{backprop_from_scratch} 
\end{quote}
	
In a nutshell, supervised learning means that training data with expected outputs is available, in contrast to unsupervised learning, where this data isn't available, but just a simple method to messure how "good" the result was. \cite{Learning2014}

Using basic calculus, the so called "error" of a network can be calculated. This is also known as solving the error minimization problem. \cite{Sathyanarayana2014}

\begin{quote}
\emph{"In the most popular version of backpropagation, called stochastic backpropagation, the weights are initially set to small random values."}\cite{Sathyanarayana2014}	
\end{quote}

With enough complexity, neural networks can represent any existing function. \cite{http://neuralnetworksanddeeplearning.com/chap4.html}


https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf