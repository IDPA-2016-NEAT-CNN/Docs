In traditional neural networks, the topology is fixed. The number of hidden layers and the number of neurons in each hidden layer are given. This makes it very easy to see the difference between two networks, since the only differences are the weights.

The downside is, that the performance of these networks heavily depends on the chosen topology, which leads to the conclusion that many networks would perform better if one had chosen a different topology.

NEAT proposes a technique to evolve the topology over time which allows the network to be better structured for a specific task then a configuration with hyper parameters.
