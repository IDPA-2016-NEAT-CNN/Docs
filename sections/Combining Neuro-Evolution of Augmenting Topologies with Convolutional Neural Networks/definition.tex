Residual Blocks are modular by nature, so they are a perfect fit for our NEAT algorithm.\\
When analyzing them, we can easily extract following parameters from them:\\
\begin{enumerate}
	\item{Weights of first dimension pooler}
	\item{Weights of convolution pooler}
	\item{Weights of second dimension pooler}
	\item{Weights of shortcut projection (if needed)}
	\item{Downscaled number of dimensions in each residual block}
	\item{Upscaled number of dimensions in each residual block}
	\item{Number of convolutions in each residual block}
	\item{Total number of residual blocks}
\end{enumerate}  
Through traditional means we can adjust the parameters 1 to 4. \\
Numbers 5 to 8 are predefined in ResNet. 
Their exact values are defined empirically and experimentally. This is of course suboptimal, as we already asserted in chapter two.

We think NEAT can optimize these by encoding them as genes in the genome. \\
However, because of the nature of our smallest building blocks, it doesn't make sense to store these genomes in a per-connection basis.\\
All parameters can be described as state of a residual block. For the last one, we just abstract it as a link to the next block. If the algorithm decides to add a new residual block, it can be inserted in a random existing link.

For the parameter tuning, we treat numbers 1 to 4 as a big vector of weights inside the genome of the residual block and apply the same chances and rules of change to them as in standard NEAT , which are:
\begin{itemize}
	\item{Chance of selecting this genome to change weights: 80\%}
	\item{Chance for each weight to be uniformly perturbed: 90\%}
	\item{Chance for each weight to be set to a random new value: 10\%}
\end{itemize}
\cite{Stanley2002}
