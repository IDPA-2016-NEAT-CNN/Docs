Residual Blocks are modular by nature, so they are a perfect fit for our NEAT algorithm.\\
When analyzing them, we can easily extract following parameters from them:\\
\begin{enumerate}
	\item{Weights of first dimension pooler}
	\item{Weights of convolution pooler}
	\item{Weights of second dimension pooler}
	\item{Weights of shortcut projection (if needed)}
	\item{Downscaled number of dimensions in each residual block}
	\item{Upscaled number of dimensions in each residual block}
	\item{Number of convolutions in each residual block}
	\item{Total number of residual blocks}
\end{enumerate}  
Through traditional means we can adjust the parameters 1 to 4. \\
Numbers 5 to 8 are predefined in ResNet. 
Their exact values are defined empirically and experimentally. This is of course suboptimal, as we already asserted in chapter two.

We think NEAT can optimize these by encoding them as genes in the genome. \\
However, because of the nature of our smallest building blocks, it doesn't make sense to store these genomes in a per-connection basis.\\
All parameters can be described as state of a residual block. For the last one, we just abstract it as a link to the next block. If the algorithm decides to add a new residual block, it can be inserted in a random existing link.

For the parameter tuning, we treat numbers 1 to 4 as a big vector of weights inside the genome of the residual block and apply the same chances and rules of change to them as in standard NEAT , which are:
\begin{itemize}
	\item{Chance of selecting this genome to change weights: 80\%}
	\item{Chance for each weight to be uniformly perturbed: 90\%}
	\item{Chance for each weight to be set to a random new value: 10\%}
\end{itemize}
\cite{Stanley2002}\\

Parameters 5 to 7 are more critical, as they greatly effect the computational cost.\\
We limited changes to be only $+1$ in each, to go with NEAT's thought of starting with a small topology and only going up if necessary. The chances are thus taken straight from how NEAT treats extra neurons: Each genome has a 3\% chance of mutating one of the mentioned parameters at birth.

Lastly, parameter 8 is the one directly in control of the network's depth. As ResNet proved, deep networks have great advantages to shallow ones \cite{KaimingHe2015}. So we made this value very prone to grow by one. The chance is analog to NEAT's chance of adding a new connection, which is 30\% at birth.

Additionally, we changed the compatibility functions $c_3$ parameter to $0.06$ to account for the higher number of potential weight differences per genome.