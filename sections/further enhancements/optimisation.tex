The single biggest challenge we faced was performance.

We had estimated that for training a full set of 800 pictures at a mere 400 to 400 pixels, we would need months for just training the network once. This held us back from efficiently mesure our algorithms correctness.

\begin{quote}
	\emph{"Currently, large-scale CNN experiments require specialized hardware, such as NVidia GPUs,
		and specialized APIs, such as NVidiaâ€™s CuDNN library, to
		achieve  adequate  training  performance."} \cite{Abuzaid2015} 
\end{quote}

Firaz Abuzaid also mentions that "at runtime,  the convolution operations are computationally expensive and take up about 67\% of the time; other estimates put this figure around 95\%".

We were (unfortunately) able to confirm these numbers as realistic - one line of code (the multiplication of the matrices values) took up to 93\% of the execution time when testing our code, the loop for executing these multiplications took another 6\% of the execution time. 

Here are some improvements that could be done to optimise the performance of convolutional neural networks:

\begin{itemize}
	\item Using GPUs to accelerate matrix-multiplications\cite{Hochberg2012}
	
	Using the power of GPUs for complex and computation heavy calculations has been very important the last years in the industry. GPU toolkits seem to consistently perform the same tasks five to ten times faster than their CPU counterparts.\cite{Abuzaid2015}
	
	\item Using the CcT method to optimize CPU usage
	
	The CcT method has proven to be up to 4 times faster than one of the often used CPU toolkits for machine learning; Caffe. Utilizing this method would allow us to improve the performance of CNNs by a big margin without having to use expensive GPUs.\cite{Abuzaid2015}
\end{itemize}

There are other approaches of optimizing CNNs to be more efficient, such as Low Rank Expansions\cite{Jaderberg2014},  the approach of Optimizing a FPGA-based Accelerator Design for Deep Convolutional Neural Networks\cite{Zhang2015} and Convexified Convolutional Neural Networks\cite{Zhang2016}.

All these approaches share the same limitations for us - it is unclear whether they are even compatible with our NEAT based evolutionary algorithm, and if they are, the changes to the inner workings of our algorithm would be drastic, so that benchmarking would be hard. Due to the recency of these developments, it is hard to fully estimate their impact onto our model and performance.